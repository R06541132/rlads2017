---
title       : 'Introducing Data Science with R'
subtitle    : 'week.8'
author      : '謝舒凱 Lab of Ontologies, Language Processing and e-Humanities'
job         : 'GIL, National Taiwan University'
logo        : lope.png
biglogo     : lopen.png
license     : by-sa
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax, quiz, bootstrap]    # {mathjax, quiz, bootstrap}
ext_widgets: {rCharts: [libraries/widgets/nvd3]}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
bibliography: /Users/shukai/LOPE_LexInfo/BIB/myRef.bib
github      : {user: loperntu, repo: dsR}



--- bg:#FFFAF0
## 大綱

1. __``Textual data manipulation``__ (80 min)
2. Group presentation (20 min)
3. Lab (50 min)






---
## 字串處理常用函數 Character manipulation

- In the area of text mining, character or string manipulation is the most important.
- `nchar()`, `substr()`, `grep()`, `grepl()`, `gsub()`, `strsplit()`, `paste()`



---
## Regular Expression

[RStudio REGEX cheat sheet](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf) 


---
## `stringr`: Character String Processing Facilities

`stringi` is an R package providing (but definitely not limiting to) equivalents of nearly all the character string processing functions known from base R. 

- string trimming, padding, and text wrapping,
- text collation (sorting and comparing), 
- extracting and replacing substrings (characters, words, and sentences), 
- text transliteration,
- others (random string generation; Unicode normalization; character encoding conversion and detection;)




---
## 中文斷詞／分詞

- *小花生了很久才出來*；*阿里巴巴創辦人馬雲端上新服務*； *可是她可是網路上最紅的人欸*
- At least two R packages: `Rwordseg` and `JiebaR`.




---
## `JiebaR`

- 先使用 `worker()` 初始化分詞引擎。

```{r, results='hide', warning=FALSE, message=FALSE}
require(jiebaR)
require(jiebaRD)
mixSeg <- worker()  
#hmmSeg <- worker(type = "hmm")
text2 <- "總有一天你會醒來，告訴我一切都是假的"
segment(text2, mixSeg)
# 或是利用分詞運算子 <=
#mixSeg <= text2
#segment(".\\data\\test.txt", mixSeg)
```

---
## `JiebaR`: 客製化 custimization

```{r,echo=TRUE, eval=FALSE}
mixSeg
# $user
# "/Library/Frameworks/R.framework/Versions/3.2/Resources/library
# /jiebaRD/dict/user.dict.utf8"
```



---
## `JiebaR`: POS tagging 
(POS tagset: [ICTCLAS 漢語詞性標註集](http://www.cnblogs.com/chenbjin/p/4341930.html))


```{r}
pos.tagger <- worker("tag")
pos.tagger <= text2
```


---
## `JiebaR`: Keywords Extraction and Similarity Calculation

- `Simhash` algorithm

```{r}
key.extract <- worker(type = "keywords", topn = 1)
key.extract <= text2
sim <- worker(type = "simhash", topn = 2)
sim <= text2
```

---
## [Exercise.1]: 魯迅:阿 Q 正傳


```{r, eval=FALSE}
luxun <- scan("http://www.gutenberg.org/files/25332/25332-0.txt",
                what="char", sep="\n")
# another lazy way
require(gutenbergr)
luxun <- gutenberg_download(25332)
luxun.seg <- segment(luxun$text, mixSeg)
write.table(luxun.seg, 'luxun.txt')
```




---
## `tidytext`: Text mining using dplyr, ggplot2, and other tidy tools

See `tidytext.r`



---
## Homework

(individual, 100 pt)
- (10 pt) Download a Chinese novel (except 魯迅:阿 Q 正傳) from Gutenberg website, clean and preprocess the text (incl. using `jiebaR()` to segment the text). 
- (50 pt) Create a sorted word-freq list. 
- (20 pt) Add a POS column (using `jiebaR()` again) to the list and write it to a file.
- (20 pt) Extract all the **pronouns** (labeled as `r`), count the occurrences separately, make the table and plot.

(group, 50 pt)
- Now you should be able to understand (most of) the R *kernels* of the [2016 US Presidential Debates](https://www.kaggle.com/mrisdal/2016-us-presidential-debates/kernels). Try to make a new one by bringing new perspective.




