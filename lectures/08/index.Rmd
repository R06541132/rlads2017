---
title       : 'Introducing Data Science with R'
subtitle    : 'week.8'
author      : '謝舒凱 Lab of Ontologies, Language Processing and e-Humanities'
job         : 'GIL, National Taiwan University'
logo        : lope.png
biglogo     : lopen.png
license     : by-sa
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax, quiz, bootstrap]    # {mathjax, quiz, bootstrap}
ext_widgets: {rCharts: [libraries/widgets/nvd3]}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
bibliography: /Users/shukai/LOPE_LexInfo/BIB/myRef.bib
github      : {user: loperntu, repo: dsR}



--- bg:#FFFAF0
## 大綱

1. __``Textual data manipulation``__ (80 min)
2. Group presentation (20 min)
3. Lab (50 min)






---
## 字串處理常用函數 Character manipulation

- In the area of text mining, character or string manipulation is the most important.
- `nchar()`, `substr()`, `grep()`, `grepl()`, `gsub()`, `strsplit()`, `paste()`
- [自學互助同樂會](https://paper.dropbox.com/doc/dsR-2016-17.-NTU-in-class-Exercise-shared-sheet-mC0isSwShE7cygCynOTgx)




---
## Textual data cleaning and transformation: An exmaple

- Separate content from metadata
```{r, eval=FALSE}
alice.web <- scan("http://www.gutenberg.org/files/11/11.txt", 
                  what="char", sep="\n")
# Explore first the alice.web, you'll get a lot of boilerplate metadata 
# so need to know where does the novel begin and end.
# e.g., head(), str(), length() >>> 2790
alice.web[1:12]
alice.web.meta <- c(alice.web[1:12])
alice.web.cont <- c(alice.web[12:2790])
length(alice.web.cont); length(alice.web)
#alice.web.cont <- paste(alice.web.cont, collapse=" ")
#length(alice.web.cont)
```




---
## Tokenize/split the text

```{r, eval=FALSE}
# first lowercase the text
alice.web.cont <- tolower(alice.web.cont)
alice.words <- strsplit(alice.web.cont, "\\W")
# detect word boundaries.
# str(alice.words) >>>> a list with one character vector with 39566 items
# use unlist() to simplify the list to a vector if needed.
alice.words <- unlist(alice.words)
```


---
## Remove the blanks

```{r, eval=FALSE}
# figure out which items in the vector are not blanks 
not.blanks.alice <- which(alice.words != "") # get a list of positions
# With the non-blanks identified, you can overwrite alice.words like this
alice.words <- alice.words[not.blanks.alice]
# A shorthand version
#alice.words <- alice.words[which(alice.words != "")]
```


---
##  Calculate the number of word tokens/types

```{r, eval=FALSE}
# calculate the count of the occurecnes of rabbit into rabbit.hits
rabbit.hits <- length(alice.words[which(alice.words == "rabbit")]) 
# calculate the count of total words into total.words
total.words <- length(alice.words)
# now divide
rabbit.hits/total.words
## calculate the number of unique word types using unique()
length(unique(alice.words))
```


---
## Vocabulary size and other distributional properties ("global statistics")

```{r, eval=FALSE}
# build a contingency table of word types and their corresponding 
# frequencies.
alice.freq <- table(alice.words)
alice.freq[1:100]
# sorted from most frequent to least frequent words
sorted.alice.freqs <- sort(alice.freq,decreasing = T)
# check whether the frequencies of the words correspond to Zipf's law.
plot(sorted.alice.freqs[1:200])
```


---
## Distribution plots
- showing where words occur across a text, i.e., where the words appear and how they behave over the course of a text.

```{r, eval=FALSE}
# First we need an integer vector (n.time) containing the positions 
# of every word in the book.
n.time <- seq(1:length(alice.words))
# Then we need to locate the position of every occurrence of "rabbit"
# in the alice.words object.
rabbit <- which(alice.words == "rabbit") 
w.count.v <- rep(NA,length(n.time))
w.count.v[rabbit] <- 1
plot(w.count.v, main="Dispersion Plot of `rabbit' in Alice",
     xlab="Novel Time", ylab="rabbit", type="h", ylim=c(0,1), yaxt='n')
```

---
## Regular Expression

[RStudio REGEX cheat sheet](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf) 


---
## `stringr`: Character String Processing Facilities

- `stringr` and `stringi`
`stringi` is an R package providing (but definitely not limiting to) equivalents of nearly all the character string processing functions known from base R. 

- string trimming, padding, and text wrapping,
- text collation (sorting and comparing), 
- extracting and replacing substrings (characters, words, and sentences), 
- text transliteration,
- others (random string generation; Unicode normalization; character encoding conversion and detection;)




---
## 中文斷詞／分詞

- *小花生了很久才出來*；*阿里巴巴創辦人馬雲端上新服務*； *可是她可是網路上最紅的人欸*
- At least two R packages: `Rwordseg` and `JiebaR`.




---
## `JiebaR`

- 先使用 `worker()` 初始化分詞引擎。

```{r, results='hide', warning=FALSE, message=FALSE}
require(jiebaR)
require(jiebaRD)
mixSeg <- worker()  
#hmmSeg <- worker(type = "hmm")
text2 <- "總有一天你會醒來，告訴我一切都是假的"
segment(text2, mixSeg)
# 或是利用分詞運算子 <=
#mixSeg <= text2
#segment(".\\data\\test.txt", mixSeg)
```

---
## `JiebaR`: 客製化 custimization

```{r,echo=TRUE, eval=FALSE}
mixSeg
# $user
# "/Library/Frameworks/R.framework/Versions/3.2/Resources/library
# /jiebaRD/dict/user.dict.utf8"
```



---
## `JiebaR`: POS tagging 
(POS tagset: [ICTCLAS 漢語詞性標註集](http://www.cnblogs.com/chenbjin/p/4341930.html))


```{r}
pos.tagger <- worker("tag")
pos.tagger <= text2
```


---
## `JiebaR`: Keywords Extraction and Similarity Calculation

- `Simhash` algorithm

```{r}
key.extract <- worker(type = "keywords", topn = 1)
key.extract <= text2
sim <- worker(type = "simhash", topn = 2)
sim <= text2
```

---
## [Exercise.1]: 魯迅:阿 Q 正傳


```{r, eval=FALSE}
luxun <- scan("http://www.gutenberg.org/files/25332/25332-0.txt",
                what="char", sep="\n")
# another lazy way
require(gutenbergr)
luxun <- gutenberg_download(25332)
luxun.seg <- segment(luxun$text, mixSeg)
write.table(luxun.seg, 'luxun.txt')
```




---
## `tidytext`: Text mining using dplyr, ggplot2, and other tidy tools

預習！文本分析必讀新書 [Tidy Text Mining with R](http://tidytextmining.com/)
See `tidytext.r`



---
## Homework

(individual, 100 pt)
- (10 pt) Download a Chinese novel (except 魯迅:阿 Q 正傳) from Gutenberg website, clean and preprocess the text (incl. using `jiebaR()` to segment the text). 
- (50 pt) Create a sorted word-freq list. 
- (20 pt) Add a POS column (using `jiebaR()` again) to the list and write it to a file.
- (20 pt) Extract all the **pronouns** (labeled as `r`), count the occurrences separately, make the table and plot.

(group, 50 pt)
- Now you should be able to understand (most of) the R *kernels* of the [2016 US Presidential Debates](https://www.kaggle.com/mrisdal/2016-us-presidential-debates/kernels). Try to make a new one by bringing new perspective.




